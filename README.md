# WORKSHOP_002 - ETL process using airflow

## Introduction

This repository presents the codes for the process for carrying out the final workflow and data analysis of workshop 002 in the ETL (Extract, Transform, Load) course. In this workshop, the construction of an ETL pipeline using Apache Airflow is addressed. The central idea is to extract information from two different data sources (a CSV file and a database), apply transformations to these data and merge them to finally store them to Google Drive in CSV format and upload them in a database.

As a last step, it is proposed to create a dashboard from the data stored in the database in order to visualize the information in the most effective and meaningful way possible.


## Challenge Overview

This challenge is an opportunity to showcase skills in data management and visualization. Data will be provided, and the objective is to demonstrate the entire ETL process using two distinct data sources and create insightful chart visualizations.

## Datasets

- **Spotify Dataset**: The Spotify dataset, which you can find [here](https://www.kaggle.com/datasets/maharshipandya/-spotify-tracks-dataset), will be read and processed using Python and Apache Airflow. Transformations and load it into a database were perform.
- **Grammys Dataset**: You can find it [here](https://www.kaggle.com/datasets/unanimad/grammy-awards). This dataset will be extracted from a mysql database (where I previously uploaded it with a code called 'load_csv_to_db.py') and will also be loaded into a database. With Apache Airflow, you will read data from the database, perform additional transformations, merge it with the Spotify dataset, load the results back into the database, and store the result into google drive.

If you are interested in exploring the details of these datasets, feel free to check them out and check my [EDA](EDA/).

## Prerequisites

Before you get started, ensure that you have the following prerequisites installed:

- Python (version 3.10.5 recommended)
- Jupyter Notebook (for exploratory data analysis)
- MySQL (for database operations)
- Looker, powerBI or Tableau (This is optional, just in case that you want to make a visualization like [this](workshop002_dashboard.pdf))

## Getting Started

1. Clone this repository to your local machine using git bash. You can use the following command.
    ```bash
    git clone https://github.com/xilenAtenea/workshop_002etl
    
    ```
2. Install python 3.x
3. Install Mysql Server 8.0 and create a database.
4. Create a virtual environment whenever you wanna work (I used WSL). You can name it as you prefer; in this case, we'll use "wenv" as an example.
    
    ```bash
    python -m venv wenv
    
    ```
    
5. Activate the virtual environment:
    - On Linux (that was my case using WSL):
    
    ```bash
    source wenv/bin/activate
    
    ```
    
6. Install the required Python packages by running the following command:
    
    ```bash
    pip install -r requirements.txt
    
    ```
    
    
7. Create database credentials file 'config_db.json' with the following structure and fill it with your own credentials:
    
    ```
    {
    "host": "",
    "user": "",
    "password": "",
    "database":""
    }
    
    ```
    
    > Note: As mentioned earlier, I loaded the database with the data using a script. You have the freedom to upload 'the_grammy_awards.csv' data from the Grammys dataset into the database in your preferred way. It's suggested to name the table 'grammys,' as I did. Please keep in mind that this database population was a separate step that you must complete as a prerequisite to enable the code in this repository to read your data successfully.
    > 
8. Download the datasets ([spotify](https://www.kaggle.com/datasets/maharshipandya/-spotify-tracks-dataset) and [grammys](https://www.kaggle.com/datasets/unanimad/grammy-awards)) from Kaggle and insert them into the '/data' folder.
9. In case that you want to check the EDA: Launch Jupyter and choose the kernel associated with the recently created virtual environment ('wenv').

Follow the code and comments in the workshop files to understand the various parts of the ETL pipeline.

## Repository Structure

### data/

This folder contains the data, i.e. all the csv. In this case it only contains merge which is the result after my own transformations, my own result. If you want to clone this repository, remember to download the previously mentioned datasets and save them in this directory.

### EDA/

Folder containing the jupyter notebooks with my exploratory data analysis and in-depth explanation of my transformations. Contains [EDA](EDA/EDA_spotify.ipynb) of the spotify dataset y [EDA](EDA/EDA_grammys.ipynb) of the grammys dataset.

### pydrive/

This folder contains files that are essential for the storage process, which was carried out using PyDrive. For more information, please refer to the 'store' section in ['etl.py'](etl.py) and check my references.

### transformations/

This folder contains the codes with the necessary functions for the transformations performed in 'etl.py'. For detailed explanations of each transformation, you can refer to the exploratory data analysis [(EDA)](EDA/) or check the comments within the code files

### etl.py and dag.py

Main codes for the purpose of this workshop.

### workshop002_dashboard.pdf

This dashboard showcases visualizations customized to my specific use case using the result data of this workshop. Its purpose is to reveal some factors that distinguish nominated and non-nominated songs, allowing us to make insightful assumptions regarding the reasons behind a song's nomination status.

## References

Throughout the workshop, you will find references and comments in the code to help you understand specific parts of the process. These references will provide additional context and guidance.

- MaharshiPandya. (2022). ðŸŽ¹ Spotify Tracks Dataset. Kaggle.com. https://www.kaggle.com/datasets/maharshipandya/-spotify-tracks-dataset/data
- Mulla, R. (2021). Exploratory Data Analysis with Pandas Python 2023 [YouTube Video]. In *YouTube*. https://www.youtube.com/watch?v=xi0vhXFPegw
- How to Plot a DataFrame using Pandas â€“ Data to Fish. (2022). Datatofish.com. https://datatofish.com/plot-dataframe-pandas/
- seaborn.scatterplot â€” seaborn 0.12.2 documentation. (2022). Pydata.org. https://seaborn.pydata.org/generated/seaborn.scatterplot.html
- How to create a seaborn correlation heatmap in Python. (2020, November 9). GeeksforGeeks; GeeksforGeeks. https://www.geeksforgeeks.org/how-to-create-a-seaborn-correlation-heatmap-in-python/
- Fontes, R. (2020). Grammy Awards. Kaggle.com. https://www.kaggle.com/datasets/unanimad/grammy-awards/data
- Zach. (2021, July 20). How to Create a Pie Chart in Seaborn - Statology. Statology. https://www.statology.org/seaborn-pie-chart/
- Maina, S. (2022, November 9). Regular Expressions (Regex) with Examples in Python and Pandas. Medium; Towards Data Science. https://towardsdatascience.com/regular-expressions-regex-with-examples-in-python-and-pandas-461228335670
- qrka. (2022, January 16). Connecting to MySQL Server in Windows Machine from WSL - DataQoil. DataQoil. https://dataqoil.com/2022/01/16/connecting-to-mysql-server-in-windows-machine-from-wsl/
- MoonCode. (2021). Aprende a usar Google Drive con Python en 20 minutos ðŸ˜ƒðŸ’»-Learn Python and Google Drive in 20 minutes [YouTube Video]. In YouTube. https://www.youtube.com/watch?v=ZI4XjwbpEwU&t=168s


Enjoy this repository and happy coding!
